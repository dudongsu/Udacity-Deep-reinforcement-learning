{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import pydicom\n",
    "import numpy as np\n",
    "# import dicom_numpy\n",
    "from os import listdir\n",
    "from scipy.io import loadmat\n",
    "#from imageio import imread, imresize, imsave\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = loadmat('Data/structset_3d_channel_all.mat')\n",
    "d = loadmat('Data/Block_3d_all.mat')\n",
    "voxel_array = f['structset_3d_channel_all']\n",
    "Block_array = d['Block_3d_all']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 256, 256, 160, 2)\n",
      "(21, 256, 256, 160)\n",
      "(21, 256, 256, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "voxel_array.shape\n",
    "#dose_array=np.expand_dims(dose_array,axis=3)\n",
    "print(voxel_array.shape)\n",
    "print(Block_array.shape)\n",
    "Block_array=np.expand_dims(Block_array,axis=4)\n",
    "test=np.concatenate((voxel_array,Block_array),axis=4)\n",
    "np.array(test)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scan(mat_path):\n",
    "    # Getting structure set images from path:\n",
    "    if not os.path.exists(mat_path):\n",
    "        print('MAT files not exists!')\n",
    "        return\n",
    "\n",
    "    f = loadmat('Data/structset_3d_channel_all.mat')\n",
    "    voxel_array = f['structset_3d_channel_all']\n",
    "   # voxel_array = np.expand_dims(voxel_array,axis=3)\n",
    "    return voxel_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Block_img(images_path):\n",
    "    # Getting dose image from file\n",
    "    if not os.path.exists(images_path):\n",
    "        print('Dose images not exists!')\n",
    "        return\n",
    "\n",
    "    f = loadmat('Data/Block_3d_all.mat')\n",
    "    Block = f['Block_3d_all']\n",
    "    Block = np.expand_dims(Block,axis=4)\n",
    "    return Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_pading(scan, seg_img, section_size):\n",
    "    # For easly split:\n",
    "    pad_size = section_size - (scan.shape[-1] % section_size)\n",
    "    if pad_size != section_size:\n",
    "        padded_scan = np.pad(scan, ((0,0),(0,0),(0,pad_size)), 'constant')\n",
    "        try:\n",
    "            padded_seg_img = np.pad(seg_img, ((0,0),(0,0),(0,pad_size)), 'constant')\n",
    "        except:\n",
    "            padded_seg_img = None\n",
    "    else:\n",
    "        padded_scan = scan\n",
    "        padded_seg_img = seg_img\n",
    "    return padded_scan, padded_seg_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_scans_imgs(scans, seg_img, section_size):\n",
    "    # Split with sliding window:\n",
    "    splitted_scans = []\n",
    "    for i in range(0, scans.shape[-1]-(section_size-1)):\n",
    "        splitted_scans.append(scans[:,:,:,i:i+section_size])\n",
    "\n",
    "    splitted_seg_img = []\n",
    "    for i in range(0, seg_img.shape[-1]-(section_size-1)):\n",
    "        splitted_seg_img.append(seg_img[:,:,i:i+section_size])\n",
    "\n",
    "    splitted_scans = np.array(splitted_scans)\n",
    "    splitted_seg_img = np.array(splitted_seg_img)\n",
    "    return splitted_scans, splitted_seg_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dataset_path, mat_file = 'structset_2d.mat', ground_file = 'doseset_2d.mat', section_size = (256, 256), test_size = 0.2, save_npy = True, dataset_save_path = 'Data/npy_dataset'):\n",
    "    # Create dateset:\n",
    "    scans, seg_imgs = [], []\n",
    "    print ('reading dataset')\n",
    "    scan = get_scan(dataset_path+'/'+mat_file)\n",
    "    Block_img = get_Block_img(dataset_path+'/'+ground_file)\n",
    "    scans = np.array(scan, dtype='float32')\n",
    "    Block_imgs = np.array(Block_img).astype('float32')\n",
    "\n",
    "    print('Scan Data Shape: ' + str(scans.shape))\n",
    "    print('Block Data Shape: ' + str(Block_imgs.shape))\n",
    "\n",
    "    if save_npy:\n",
    "        if not os.path.exists(dataset_save_path):\n",
    "            os.makedirs(dataset_save_path)\n",
    "        np.save(dataset_save_path+'/scans.npy', scans)\n",
    "        np.save(dataset_save_path+'/Block.npy', Block_imgs)\n",
    "        print('NPY dataset saved!')\n",
    "\n",
    "    X, X_test, Y, Y_test = train_test_split(scans, Block_imgs, test_size=test_size, random_state=42)\n",
    "    print('training data size'+ str(X.shape))\n",
    "    print('test data size'+ str(Y.shape))\n",
    "    return X, X_test, Y, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_npy_dataset(npy_dataset_path, split_npy_dataset_path, test_path, batch_size, test_size):\n",
    "    X = np.load(npy_dataset_path+'/scans.npy')\n",
    "    Y = np.load(npy_dataset_path+'/Block.npy')\n",
    "\n",
    "    if not os.path.exists(split_npy_dataset_path):\n",
    "        os.makedirs(split_npy_dataset_path)\n",
    "    if not os.path.exists(test_path):\n",
    "        os.makedirs(test_path)\n",
    "\n",
    "    X, X_test, Y, Y_test = train_test_split(X, Y, test_size=test_size, random_state=42)\n",
    "    print('X_test size',X_test.shape,'Y_test size=',Y_test.shape)\n",
    "    test_npy = np.concatenate((X_test,Y_test),axis=4)\n",
    "    test_npy = np.array(test_npy)\n",
    "\n",
    "   # np.save(test_path+'/test.npy', test_npy)\n",
    "    for batch_i in range(0, Y_test.shape[0], batch_size):\n",
    "        batch_npy = np.concatenate((X_test[batch_i:batch_i+batch_size],Y_test[batch_i:batch_i+batch_size]),axis=4)\n",
    "        batch_npy = np.array(batch_npy)\n",
    "        np.save(test_path+'/batch_{0}.npy'.format(batch_i), batch_npy)\n",
    "\n",
    "    for batch_i in range(0, Y.shape[0], batch_size):\n",
    "        batch_npy = np.concatenate((X[batch_i:batch_i+batch_size],Y[batch_i:batch_i+batch_size]),axis=4)\n",
    "        batch_npy = np.array(batch_npy)\n",
    "        np.save(split_npy_dataset_path+'/batch_{0}.npy'.format(batch_i), batch_npy)\n",
    "\n",
    "        \n",
    "    print('Splitted NPY Dataset saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_npy_dataset(npy_dataset_path, test_size = 0.2):\n",
    "    X = np.load(npy_dataset_path+'/scans.npy')\n",
    "    Y = np.load(npy_dataset_path+'/dose.npy')\n",
    "    X, X_test, Y, Y_test = train_test_split(X, Y, test_size=test_size, random_state=42)\n",
    "    print('Train Data Shape: ' + str(X.shape[0]))\n",
    "    print('Test Data Shape: ' + str(X_test.shape[0]))\n",
    "    return X, X_test, Y, Y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading dataset\n",
      "Scan Data Shape: (21, 256, 256, 160, 2)\n",
      "Block Data Shape: (21, 256, 256, 160, 1)\n",
      "NPY dataset saved!\n",
      "training data size(16, 256, 256, 160, 2)\n",
      "test data size(16, 256, 256, 160, 1)\n",
      "X_test size (5, 256, 256, 160, 2) Y_test size= (5, 256, 256, 160, 1)\n",
      "Splitted NPY Dataset saved!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    dataset_path = 'Data'\n",
    "    npy_dataset_path = 'Data/npy_dataset'\n",
    "    splitted_npy_dataset_path = npy_dataset_path+'/splitted_npy_dataset'\n",
    "    test_path = npy_dataset_path+'/test_npy'\n",
    "\n",
    "    X, X_test, Y, Y_test = get_dataset(dataset_path, mat_file = 'structset_3d_channel_all.mat', ground_file = 'Block_3d_all.mat', section_size = (256, 256, 4), test_size = 0.2, save_npy = True, dataset_save_path = npy_dataset_path)\n",
    "    split_npy_dataset(npy_dataset_path, splitted_npy_dataset_path, test_path, batch_size = 1, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = get_scan('Data/structset.mat')\n",
    "test2  = get_dose_img('Data/doseset.mat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test, Y, Y_test = train_test_split(test1, test2, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
